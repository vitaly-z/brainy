[package]
name = "candle-embeddings"
version = "0.1.0"
edition = "2021"
description = "WASM-based sentence embeddings using Candle and all-MiniLM-L6-v2"
license = "MIT"

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
# Candle ML framework
candle-core = "0.8"
candle-nn = "0.8"
candle-transformers = "0.8"

# HuggingFace tokenizer with WASM support
# Use unstable_wasm feature which provides fancy-regex instead of onig
tokenizers = { version = "0.20", default-features = false, features = ["unstable_wasm"] }

# WASM bindings
wasm-bindgen = "0.2"
wasm-bindgen-futures = "0.4"
js-sys = "0.3"
web-sys = { version = "0.3", features = ["console"] }

# Serialization for model loading
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Error handling
anyhow = "1.0"

# Async
futures = "0.3"

# WASM compatibility - force getrandom with js/wasm_js features
# getrandom 0.2 (from tokenizers->rand) needs "js" feature
# getrandom 0.3 (from candle->rand 0.9) needs "wasm_js" feature + rustflags
[target.'cfg(target_arch = "wasm32")'.dependencies]
getrandom_02 = { package = "getrandom", version = "0.2", features = ["js"] }
getrandom = { version = "0.3", features = ["wasm_js"] }

[dev-dependencies]
wasm-bindgen-test = "0.3"

[profile.release]
opt-level = 3       # Optimize for speed (~15-20% faster inference, ~1MB larger binary)
lto = true          # Link-time optimization
codegen-units = 1   # Single codegen unit for better optimization
panic = "abort"     # Abort on panic (smaller binary)

[features]
default = []
simd = []  # Enable SIMD when browser support is available
