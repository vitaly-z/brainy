# Brainy v3 â†’ v4.0.0 Migration Guide

> **Migration Complexity**: Low
> **Breaking Changes**: None (fully backward compatible)
> **New Features**: Lifecycle management, batch operations, compression, quota monitoring

## Overview

Brainy v4.0.0 is a **backward-compatible** release focused on production-ready cost optimization features. Your existing v3 code will continue to work without modifications, but you'll want to enable the new v4.0.0 features for significant cost savings.

**Key Benefits of Upgrading:**
- ðŸ’° **96% cost savings** with lifecycle policies
- ðŸš€ **1000x faster** bulk deletions with batch operations
- ðŸ“¦ **60-80% space savings** with gzip compression
- ðŸ“Š **Real-time quota monitoring** for OPFS
- ðŸŽ¯ **Zero downtime** migration

## What's New in v4.0.0

### 1. Lifecycle Management (Cloud Storage)

**Automatic tier transitions for massive cost savings:**

```typescript
// NEW in v4.0.0
await storage.setLifecyclePolicy({
  rules: [{
    id: 'archive-old-data',
    prefix: 'entities/',
    status: 'Enabled',
    transitions: [
      { days: 30, storageClass: 'STANDARD_IA' },
      { days: 90, storageClass: 'GLACIER' }
    ]
  }]
})
```

**Supported on:**
- âœ… AWS S3 (Lifecycle + Intelligent-Tiering)
- âœ… Google Cloud Storage (Lifecycle + Autoclass)
- âœ… Azure Blob Storage (Lifecycle policies)

### 2. Batch Operations

**1000x faster bulk deletions:**

```typescript
// v3: Delete one at a time (slow, expensive)
for (const id of idsToDelete) {
  await brain.delete(id)  // 1000 API calls for 1000 entities
}

// v4.0.0: Batch delete (fast, cheap)
const paths = idsToDelete.flatMap(id => [
  `entities/nouns/vectors/${id.substring(0, 2)}/${id}.json`,
  `entities/nouns/metadata/${id.substring(0, 2)}/${id}.json`
])
await storage.batchDelete(paths)  // 1 API call for 1000 objects (S3)
```

**Efficiency gains:**
- S3: 1000 objects per batch
- GCS: 100 objects per batch
- Azure: 256 objects per batch

### 3. Compression (FileSystem)

**60-80% space savings for local storage:**

```typescript
// NEW in v4.0.0
const brain = new Brainy({
  storage: {
    type: 'filesystem',
    path: './data',
    compression: true  // Enable gzip compression
  }
})

// Automatic compression/decompression on all reads/writes
```

### 4. Quota Monitoring (OPFS)

**Prevent quota exceeded errors in browsers:**

```typescript
// NEW in v4.0.0
const status = await storage.getStorageStatus()

if (status.details.usagePercent > 80) {
  console.warn('Approaching quota limit:', status.details)
  // Take action: cleanup old data, notify user, etc.
}
```

### 5. Tier Management (Azure)

**Manual or automatic tier transitions:**

```typescript
// NEW in v4.0.0
await storage.changeBlobTier(blobPath, 'Cool')  // Hot â†’ Cool (50% savings)
await storage.batchChangeTier([blob1, blob2], 'Archive')  // 99% savings

// Rehydrate from Archive when needed
await storage.rehydrateBlob(blobPath, 'High')  // 1-hour rehydration
```

## Storage Architecture Changes

### v3.x Storage Structure

```
brainy-data/
â”œâ”€â”€ nouns/
â”‚   â””â”€â”€ {uuid}.json          # Single file per entity
â”œâ”€â”€ verbs/
â”‚   â””â”€â”€ {uuid}.json          # Single file per relationship
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ __metadata_*.json    # Indexes
â””â”€â”€ _system/
    â””â”€â”€ statistics.json
```

### v4.0.0 Storage Structure (Automatic Migration)

```
brainy-data/
â”œâ”€â”€ entities/
â”‚   â”œâ”€â”€ nouns/
â”‚   â”‚   â”œâ”€â”€ vectors/         # Vector + HNSW graph (NEW)
â”‚   â”‚   â”‚   â”œâ”€â”€ 00/ ... ff/  # 256 UUID shards (NEW)
â”‚   â”‚   â””â”€â”€ metadata/        # Business data (NEW)
â”‚   â”‚       â”œâ”€â”€ 00/ ... ff/  # 256 UUID shards (NEW)
â”‚   â””â”€â”€ verbs/
â”‚       â”œâ”€â”€ vectors/         # Relationship vectors (NEW)
â”‚       â”‚   â”œâ”€â”€ 00/ ... ff/
â”‚       â””â”€â”€ metadata/        # Relationship data (NEW)
â”‚           â”œâ”€â”€ 00/ ... ff/
â””â”€â”€ _system/                 # Unchanged
    â””â”€â”€ __metadata_*.json
```

**Key Changes:**
1. **Metadata/Vector Separation**: Entities split into 2 files for optimal I/O
2. **UUID-Based Sharding**: 256 shards for cloud storage optimization
3. **Automatic Migration**: Brainy handles migration transparently on first run

## Migration Steps

### Step 1: Update Brainy Package

```bash
npm install @soulcraft/brainy@latest
```

**Check your version:**
```bash
npm list @soulcraft/brainy
# Should show: @soulcraft/brainy@4.0.0
```

### Step 2: No Code Changes Required! âœ…

Your existing v3 code will work without modifications:

```typescript
// This v3 code works perfectly in v4.0.0
const brain = new Brainy({
  storage: { type: 'filesystem', path: './data' }
})

await brain.init()
await brain.add("content", { type: "entity" })
const results = await brain.search("query")
```

### Step 3: First Run (Automatic Migration)

On first initialization with v4.0.0:

1. **Brainy detects v3 storage structure**
2. **Transparently migrates to v4.0.0 structure**:
   - Creates `entities/` directory
   - Migrates `nouns/` â†’ `entities/nouns/vectors/` + `entities/nouns/metadata/`
   - Migrates `verbs/` â†’ `entities/verbs/vectors/` + `entities/verbs/metadata/`
   - Applies UUID-based sharding
3. **Old structure preserved** (optional cleanup later)

**Migration time:**
- 10K entities: ~1 minute
- 100K entities: ~10 minutes
- 1M entities: ~2 hours

**Zero downtime:**
- Migration happens during init()
- No data loss
- Automatic rollback on error

### Step 4: Enable v4.0.0 Features (Optional but Recommended)

#### Enable Lifecycle Policies (Cloud Storage)

**AWS S3:**
```typescript
// After init()
await storage.setLifecyclePolicy({
  rules: [{
    id: 'optimize-storage',
    prefix: 'entities/',
    status: 'Enabled',
    transitions: [
      { days: 30, storageClass: 'STANDARD_IA' },
      { days: 90, storageClass: 'GLACIER' }
    ]
  }]
})

// Or use Intelligent-Tiering (recommended)
await storage.enableIntelligentTiering('entities/', 'auto-optimize')
```

**Google Cloud Storage:**
```typescript
await storage.enableAutoclass({
  terminalStorageClass: 'ARCHIVE'
})
```

**Azure Blob Storage:**
```typescript
await storage.setLifecyclePolicy({
  rules: [{
    name: 'optimize-blobs',
    enabled: true,
    type: 'Lifecycle',
    definition: {
      filters: { blobTypes: ['blockBlob'] },
      actions: {
        baseBlob: {
          tierToCool: { daysAfterModificationGreaterThan: 30 },
          tierToArchive: { daysAfterModificationGreaterThan: 90 }
        }
      }
    }
  }]
})
```

#### Enable Compression (FileSystem)

```typescript
const brain = new Brainy({
  storage: {
    type: 'filesystem',
    path: './data',
    compression: true  // NEW: 60-80% space savings
  }
})
```

#### Use Batch Operations

```typescript
// Replace individual deletes with batch delete
const idsToDelete = [/* ... */]
const paths = idsToDelete.flatMap(id => {
  const shard = id.substring(0, 2)
  return [
    `entities/nouns/vectors/${shard}/${id}.json`,
    `entities/nouns/metadata/${shard}/${id}.json`
  ]
})

await storage.batchDelete(paths)  // Much faster!
```

#### Monitor Quota (OPFS)

```typescript
// Periodically check quota in browser apps
setInterval(async () => {
  const status = await storage.getStorageStatus()
  if (status.details.usagePercent > 80) {
    notifyUser('Storage approaching limit')
  }
}, 60000)  // Check every minute
```

## Backward Compatibility

### Guaranteed to Work (No Changes Needed)

âœ… All v3 APIs remain unchanged
âœ… Storage adapters backward compatible
âœ… Metadata structure unchanged
âœ… Query APIs unchanged
âœ… Configuration options unchanged

### New Optional APIs (Add When Ready)

- `storage.setLifecyclePolicy()` - NEW in v4.0.0
- `storage.getLifecyclePolicy()` - NEW in v4.0.0
- `storage.removeLifecyclePolicy()` - NEW in v4.0.0
- `storage.enableIntelligentTiering()` - NEW in v4.0.0 (S3)
- `storage.enableAutoclass()` - NEW in v4.0.0 (GCS)
- `storage.batchDelete()` - NEW in v4.0.0
- `storage.changeBlobTier()` - NEW in v4.0.0 (Azure)
- `storage.getStorageStatus()` - Enhanced in v4.0.0

## Testing Your Migration

### 1. Test in Development First

```typescript
// Create test brain with v4.0.0
const testBrain = new Brainy({
  storage: { type: 'filesystem', path: './test-data' }
})

await testBrain.init()

// Verify migration
console.log('Initialization complete')

// Test basic operations
const id = await testBrain.add("test content", { type: "test" })
const results = await testBrain.search("test")
console.log('Basic operations working:', results.length > 0)
```

### 2. Verify Storage Structure

```bash
# Check new directory structure
ls -la ./test-data/entities/nouns/vectors/
# Should see: 00/ 01/ 02/ ... ff/ (256 shards)

ls -la ./test-data/entities/nouns/metadata/
# Should see: 00/ 01/ 02/ ... ff/ (256 shards)
```

### 3. Verify Data Integrity

```typescript
// Query all entities
const allEntities = await testBrain.find({})
console.log('Total entities:', allEntities.length)

// Verify specific entities
const entity = await testBrain.get(knownEntityId)
console.log('Entity retrieved:', entity !== null)
```

### 4. Test Performance

```typescript
// Benchmark search
const start = Date.now()
const results = await testBrain.search("query")
const duration = Date.now() - start
console.log('Search time:', duration, 'ms')

// Should be similar or faster than v3
```

## Rollback Procedure (If Needed)

If you encounter issues, you can rollback:

### Option 1: Rollback Package

```bash
# Reinstall v3
npm install @soulcraft/brainy@^3.50.0

# Restart application
```

**Important:** v3 can still read v3-structured data (preserved during migration)

### Option 2: Restore from Backup

```bash
# If you backed up data before migration
rm -rf ./data
cp -r ./data-backup ./data

# Reinstall v3
npm install @soulcraft/brainy@^3.50.0
```

## Common Migration Scenarios

### Scenario 1: Small Application (<10K Entities)

**Migration time:** 1 minute
**Recommended approach:**
1. Update npm package
2. Restart application (automatic migration)
3. Enable lifecycle policies immediately

### Scenario 2: Medium Application (10K-1M Entities)

**Migration time:** 10 minutes - 2 hours
**Recommended approach:**
1. Backup data
2. Update npm package
3. Schedule maintenance window
4. Restart application (automatic migration)
5. Verify data integrity
6. Enable lifecycle policies

### Scenario 3: Large Application (1M+ Entities)

**Migration time:** 2-24 hours
**Recommended approach:**
1. **Backup data** (critical!)
2. Test migration on staging environment
3. Schedule extended maintenance window
4. Update npm package on production
5. Restart application (automatic migration)
6. Monitor migration progress
7. Verify data integrity thoroughly
8. Enable lifecycle policies gradually

### Scenario 4: Multi-Node Distributed System

**Recommended approach:**
1. Perform blue-green deployment:
   - Keep v3 nodes running (blue)
   - Deploy v4 nodes (green)
   - Migrate data once
   - Switch traffic to v4 nodes
   - Decommission v3 nodes

## Cost Savings After Migration

### Enable All v4.0.0 Features

**500TB Dataset Example:**

**Before v4.0.0 (v3 with AWS S3 Standard):**
```
Storage: $138,000/year
Operations: $5,000/year
Total: $143,000/year
```

**After v4.0.0 (with Intelligent-Tiering):**
```
Storage: $51,000/year (64% savings)
Operations: $5,000/year
Total: $56,000/year
```

**After v4.0.0 (with Lifecycle Policies):**
```
Storage: $5,940/year (96% savings!)
Operations: $5,000/year
Total: $10,940/year
```

**Annual Savings: $132,060 (96% reduction)**

## Troubleshooting

### Issue: Migration takes too long

**Solution:**
- Migration is I/O bound
- For 1M+ entities, consider:
  - Running during off-peak hours
  - Using faster storage (SSD vs HDD)
  - Increasing available memory
  - Running on more powerful instance

### Issue: "Storage structure not recognized"

**Solution:**
```typescript
// Manually trigger migration
await brain.storage.migrateToV4()  // If automatic migration fails

// Or start fresh (data loss warning!)
await brain.storage.clear()
await brain.init()
```

### Issue: Lifecycle policy not working

**Solution:**
```typescript
// Verify policy is set
const policy = await storage.getLifecyclePolicy()
console.log('Active rules:', policy.rules)

// Cloud providers may take 24-48 hours to start transitions
// Check again after 2 days

// Verify in cloud console:
// - AWS: S3 â†’ Bucket â†’ Management â†’ Lifecycle
// - GCS: Storage â†’ Bucket â†’ Lifecycle
// - Azure: Storage Account â†’ Lifecycle management
```

### Issue: Batch delete not working

**Solution:**
```typescript
// Ensure storage adapter supports batch delete
const status = await storage.getStorageStatus()
console.log('Storage type:', status.type)

// Batch delete requires:
// - S3CompatibleStorage âœ…
// - GcsStorage âœ…
// - AzureBlobStorage âœ…
// - FileSystemStorage âœ…
// - OPFSStorage âœ…
// - MemoryStorage âœ…
```

## Best Practices

1. âœ… **Backup before upgrading** (especially for large datasets)
2. âœ… **Test on staging first** (verify migration works)
3. âœ… **Monitor during migration** (watch logs for errors)
4. âœ… **Enable lifecycle policies immediately** (start saving costs)
5. âœ… **Use batch operations** (for any bulk cleanup)
6. âœ… **Monitor quota** (OPFS browser apps)
7. âœ… **Enable compression** (FileSystem storage)

## Getting Help

**Documentation:**
- [AWS S3 Cost Optimization Guide](./operations/cost-optimization-aws-s3.md)
- [GCS Cost Optimization Guide](./operations/cost-optimization-gcs.md)
- [Azure Cost Optimization Guide](./operations/cost-optimization-azure.md)
- [Cloudflare R2 Cost Optimization Guide](./operations/cost-optimization-cloudflare-r2.md)

**Support:**
- GitHub Issues: [https://github.com/soulcraft/brainy/issues](https://github.com/soulcraft/brainy/issues)
- GitHub Discussions: [https://github.com/soulcraft/brainy/discussions](https://github.com/soulcraft/brainy/discussions)

## Summary

**Migration Checklist:**
- âœ… Backup data
- âœ… Update npm package (`npm install @soulcraft/brainy@latest`)
- âœ… Restart application (automatic migration)
- âœ… Verify data integrity
- âœ… Enable lifecycle policies
- âœ… Enable compression (FileSystem)
- âœ… Use batch operations
- âœ… Monitor cost savings

**Expected Results:**
- âœ… Zero downtime migration
- âœ… Full backward compatibility
- âœ… 60-96% cost savings
- âœ… 1000x faster bulk operations
- âœ… 60-80% space savings (with compression)

**Timeline:**
- Small app (<10K): 1 minute migration
- Medium app (10K-1M): 10 minutes - 2 hours
- Large app (1M+): 2-24 hours

**Welcome to Brainy v4.0.0! ðŸŽ‰**

---

**Version**: v4.0.0
**Migration Difficulty**: Low
**Breaking Changes**: None
**Recommended Upgrade**: Yes (significant cost savings)
